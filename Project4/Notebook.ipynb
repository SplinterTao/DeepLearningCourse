{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from typing import Tuple, Callable, Dict\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "import torchvision\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'processed_celeba_small/celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845af555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(size):\n",
    "    \"\"\" Transforms to apply to the image.\"\"\"\n",
    "    # TODO: edit this function by appening transforms to the below list\n",
    "    transforms = [Resize(size),ToTensor(),Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "\n",
    "    \n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DatasetDirectory(Dataset):\n",
    "    def __init__(self, directory: str, transforms: Callable = None, extension: str = '.jpg'):\n",
    "        self.directory=directory\n",
    "        self.extension=extension\n",
    "        self.transforms=transforms\n",
    "    def __len__(self) -> int:\n",
    "        count=0\n",
    "        for files in os.listdir(self.directory):\n",
    "            \n",
    "            if files.endswith(self.extension):\n",
    "                count+=1\n",
    "        return count\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\" load an image and apply transformation \"\"\"\n",
    "        filenameslist=[]\n",
    "        for files in os.listdir(self.directory):\n",
    "            if files.endswith(self.extension):\n",
    "                filenameslist.append(files)\n",
    "        name_to_read=self.directory+filenameslist[index]\n",
    "        image=PIL.Image.open(name_to_read)\n",
    "        function=get_transforms(64)\n",
    "        tensor_image=function(image)\n",
    "        \n",
    "        return tensor_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# run this cell to verify your dataset implementation\n",
    "dataset = DatasetDirectory(data_dir, get_transforms((64, 64)))\n",
    "tests.check_dataset_outputs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2583e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDirectory(data_dir, get_transforms((64, 64)))\n",
    "tests.check_dataset_outputs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(images):\n",
    "    \"\"\"Transform images from [-1.0, 1.0] to [0, 255] and cast them to uint8.\"\"\"\n",
    "    return ((images + 1.) / 2. * 255).astype(np.uint8)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "    img = dataset[idx].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = denormalize(img)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module,Sequential, Flatten, Linear, LeakyReLU\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9ec264",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b9/w4wr95zj6xd_8wgpnqk78b2m0000gn/T/ipykernel_9856/3355593760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mconvolutional\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmade\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConv\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchNorm\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconv\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block is made of 3 layers: Conv -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - batch_norm: whether to use batch norm or not\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, batch_norm: bool = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(Module):\n",
    "\n",
    "    def __init__(self, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        \n",
    "        self.conv1 = ConvBlock(3, conv_dim, 4, batch_norm=False) # first layer, no batch_norm\n",
    "        \n",
    "        self.conv2 = ConvBlock(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        self.conv3 = ConvBlock(conv_dim*2, conv_dim*4, 4)\n",
    "        self.conv4 = ConvBlock(conv_dim*4, conv_dim*8, 4)\n",
    "        self.conv5 = ConvBlock(conv_dim*8, conv_dim*16, 4)\n",
    "        self.conv6 = ConvBlock(conv_dim*16, 1, 4, batch_norm=False)\n",
    "        \n",
    "        #self.flatten = torch.nn.Flatten()\n",
    "        # final, fully-connected layer\n",
    "        self.fc = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # all hidden layers + leaky relu activation\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv4(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv5(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        #print(x.shape)\n",
    "        # final output layer\n",
    "        x = self.fc(x)\n",
    "        #x=torch.reshape(x,[-1,1,1,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61438f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "tests.check_discriminator(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(Module):\n",
    "    \"\"\"\n",
    "    A \"de-convolutional\" block is made of 3 layers: ConvTranspose -> BatchNorm -> Activation.\n",
    "    args:\n",
    "    - in_channels: number of channels in the input to the conv layer\n",
    "    - out_channels: number of filters in the conv layer\n",
    "    - kernel_size: filter dimension of the conv layer\n",
    "    - stride: stride of the conv layer\n",
    "    - padding: padding of the conv layer\n",
    "    - batch_norm: whether to use batch norm or not\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int,\n",
    "                 padding: int,\n",
    "                 batch_norm: bool = False):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.deconv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator model adapted from DCGAN\n",
    "    args:\n",
    "    - latent_dim: dimension of the latent vector\n",
    "    - conv_dim: control the number of filters in the convtranspose layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, conv_dim: int = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        # transpose conv layers\n",
    "        self.deconv1 = DeconvBlock(in_channels=latent_dim, \n",
    "                                   out_channels=conv_dim*8, kernel_size=4, \n",
    "                                   stride=1, padding=0)\n",
    "        self.deconv2 = DeconvBlock(conv_dim*8, conv_dim*4, 4, 2, 1)\n",
    "        \n",
    "        self.deconv3 = DeconvBlock(conv_dim*4, conv_dim*2, 4, 2, 1)\n",
    "        \n",
    "        self.deconv4 = DeconvBlock(conv_dim*2, conv_dim*1, 4, 2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.deconv5 = nn.ConvTranspose2d(conv_dim, 3, 4, stride=2, padding=1)\n",
    "        self.last_activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.deconv1(x)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        \n",
    "        x = self.deconv4(x)\n",
    "        \n",
    "        x=self.deconv5(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to verify your generator implementation\n",
    "latent_dim = 128\n",
    "generator = Generator(latent_dim)\n",
    "tests.check_generator(generator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3dac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea908011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "def create_optimizers(generator: Module, discriminator: Module,lr,beta1,beta2):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr, [beta1,beta2])\n",
    "    d_optimizer =optim.Adam(discriminator.parameters(), lr, [beta1,beta2])\n",
    "    return g_optimizer, d_optimizer\n",
    "\n",
    "\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.ones(batch_size).cuda()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(D_out.squeeze().cuda(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size).cuda() # fake labels = 0\n",
    "    criterion_fake_loss = nn.BCELoss()\n",
    "    loss = criterion_fake_loss(D_out.squeeze().cuda(), labels)\n",
    "    return loss\n",
    "\n",
    "def generator_loss(fake_logits):\n",
    "    return real_loss(fake_logits)\n",
    "\n",
    "def discriminator_loss(real_logits,fake_logits):\n",
    "    return fake_loss(fake_logits)+real_loss(real_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\" This function enforces \"\"\"\n",
    "    gp = 0\n",
    "    # TODO (Optional): implement the gradient penalty\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_step(batch_size: int, latent_dim: int):\n",
    "    \"\"\" One training step of the generator. \"\"\"\n",
    "    noise=torch.randn(batch_size,latent_dim,1,1).cuda()\n",
    "    fake_images=generator(noise)\n",
    "    generator.zero_grad()\n",
    "    fake_logits=discriminator(fake_images)\n",
    "    g_loss=generator_loss(fake_logits)\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return {'loss': g_loss,\"fake_logits\":fake_logits}\n",
    "    #return g_loss\n",
    "\n",
    "\n",
    "def discriminator_step(batch_size: int, latent_dim: int, real_images: torch.Tensor):\n",
    "    \"\"\" One training step of the discriminator. \"\"\"\n",
    "    # TODO: implement the discriminator step (foward pass, loss calculation and backward pass)\n",
    "    discriminator.zero_grad()\n",
    "    real_logits=discriminator(real_images)\n",
    "    fake_images=generator(torch.randn(batch_size,latent_dim,1,1).cuda())\n",
    "    fake_logits=discriminator(fake_images.detach()) \n",
    "    d_loss=discriminator_loss(real_logits,fake_logits)\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return {'loss': d_loss}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "latent_dim = 128\n",
    "device = 'cuda'\n",
    "n_epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print_every = 10\n",
    "\n",
    "# Create optimizers for the discriminator D and generator G\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, discriminator,lr=0.0002,beta1=0.5,beta2=0.999)\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=64, \n",
    "                        shuffle=True, \n",
    "                        num_workers=4, \n",
    "                        drop_last=True,\n",
    "                        pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[27]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def display(fixed_latent_vector: torch.Tensor):\n",
    "    \"\"\" helper function to display images during training \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    plot_size = 16\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        img = fixed_latent_vector[idx, ...].detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = denormalize(img)\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent_vector = torch.randn(16, latent_dim, 1, 1).float().cuda()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_i, real_images in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "     \n",
    "        ####################################\n",
    "        g_loss=generator_step(batch_size,latent_dim)\n",
    "        \n",
    "        d_loss=discriminator_step(batch_size,latent_dim,real_images)\n",
    "        ####################################\n",
    "        \n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            d = d_loss['loss'].item()\n",
    "            g = g_loss['loss'].item()\n",
    "            losses.append((d, g))\n",
    "            # print discriminator and generator loss\n",
    "            time = str(datetime.now()).split('.')[0]\n",
    "            print(f'{time} | Epoch [{epoch+1}/{n_epochs}] | Batch {batch_i}/{len(dataloader)} | d_loss: {d:.4f} | g_loss: {g:.4f}')\n",
    "    \n",
    "    # display images during training\n",
    "    generator.eval()\n",
    "    generated_images = generator(fixed_latent_vector)\n",
    "    display(generated_images)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab186ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[52]:\n",
    "\n",
    "\n",
    "nn.BCE\n",
    "\n",
    "\n",
    "# ### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "# When you answer this question, consider the following factors:\n",
    "# * The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "# * Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "# * Optimization strategy; optimizers and number of epochs affect your final result\n",
    "# * Loss functions\n",
    "\n",
    "# **Answer:** (Write your answer in this cell)\n",
    "\n",
    "# ### Submitting This Project\n",
    "# When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\".  \n",
    "# \n",
    "# Submit the notebook using the ***SUBMIT*** button in the bottom right corner of the Project Workspace.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "loss=torch.nn.BCEWithLogitsLoss()\n",
    "loss(torch.Tensor([-1]),torch.Tensor([1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
